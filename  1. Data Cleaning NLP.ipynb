{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOWERCASING -- lOWER CASE ALL THE LETTERS IN ANY DATASET SO THAT EVERY SAME WORD IS NOT COUNTED TWICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Text\n",
      "0      the quick brown fox\n",
      "1  jumps over the lazy dog\n",
      "2              hello world\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe with a column containing text data\n",
    "data = {'Text': [\"The Quick Brown Fox\", \"Jumps Over The Lazy Dog\", \"Hello World\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Lowercase the entire 'Text' column\n",
    "df['Text'] = df['Text'].str.lower()\n",
    "\n",
    "# Print the dataframe to see the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove html tags -- they do not provide any meaning to a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is HTML text with tags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample HTML text\n",
    "html_text = \"<p>This is <strong>HTML</strong> text with <a href='#'>tags</a>.</p>\"\n",
    "\n",
    "# Remove HTML tags using regular expressions\n",
    "plain_text = re.sub(r'<[^>]+>', '', html_text)\n",
    "\n",
    "# Print the plain text without HTML tags\n",
    "print(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can try the below function as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           HTML_Text  \\\n",
      "0  <p>This is <strong>HTML</strong> text with <a ...   \n",
      "1  <p>Another <em>example</em> with <a href='#'>l...   \n",
      "2  <p>Yet <u>another</u> <strong>example</strong>...   \n",
      "\n",
      "                     Plain_Text  \n",
      "0  This is HTML text with tags.  \n",
      "1   Another example with links.  \n",
      "2          Yet another example.  \n"
     ]
    }
   ],
   "source": [
    "# Sample dataframe with a column containing HTML text\n",
    "data = {'HTML_Text': [\"<p>This is <strong>HTML</strong> text with <a href='#'>tags</a>.</p>\",\n",
    "                      \"<p>Another <em>example</em> with <a href='#'>links</a>.</p>\",\n",
    "                      \"<p>Yet <u>another</u> <strong>example</strong>.</p>\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to remove HTML tags from a string\n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
    "    return clean_text\n",
    "\n",
    "# Apply remove_html_tags function to the entire 'HTML_Text' column\n",
    "df['Plain_Text'] = df['HTML_Text'].apply(remove_html_tags)\n",
    "\n",
    "# Print the dataframe with plain text\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Text_with_URLs         Text_without_URLs\n",
      "0  Check out this website: https://www.example.com  Check out this website: \n",
      "1        Visit our site at www.another-example.com        Visit our site at \n",
      "2        More info available at http://example.org   More info available at \n"
     ]
    }
   ],
   "source": [
    "# Sample dataframe with a column containing text with URLs\n",
    "data = {'Text_with_URLs': [\"Check out this website: https://www.example.com\",\n",
    "                           \"Visit our site at www.another-example.com\",\n",
    "                           \"More info available at http://example.org\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to remove URLs from a string\n",
    "def remove_urls(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    clean_text = re.sub(url_pattern, '', text)\n",
    "    return clean_text\n",
    "\n",
    "# Apply remove_urls function to the entire column\n",
    "df['Text_without_URLs'] = df['Text_with_URLs'].apply(remove_urls)\n",
    "\n",
    "# Print the dataframe with text without URLs\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is a sample text\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Sample text containing punctuation\n",
    "text_with_punctuation = \"Hello, world! This is a sample text.\"\n",
    "\n",
    "# Remove punctuation using string.punctuation\n",
    "text_without_punctuation = ''.join(char for char in text_with_punctuation if char not in string.punctuation)\n",
    "\n",
    "# Print text without punctuation\n",
    "print(text_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Text_with_Punctuation  \\\n",
      "0                            Hello, world!   \n",
      "1  This is a sample text with punctuation!   \n",
      "2                             How are you?   \n",
      "\n",
      "                 Text_without_Punctuation  \n",
      "0                             Hello world  \n",
      "1  This is a sample text with punctuation  \n",
      "2                             How are you  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Sample DataFrame with a column containing text with punctuation\n",
    "data = {'Text_with_Punctuation': [\"Hello, world!\", \"This is a sample text with punctuation!\", \"How are you?\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to remove punctuation from a string\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "# Apply the remove_punctuation function to the entire column\n",
    "df['Text_without_Punctuation'] = df['Text_with_Punctuation'].apply(remove_punctuation)\n",
    "\n",
    "# Print the DataFrame with the updated column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {'AFK': 'Away from Keyboard', 'FYI': 'For your Information','u2' : 'you too'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For your Information'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('FYI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "emogi_text = \"What does the üçÜ üí¶ emoji mean?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What does the üçÜ üí¶ emoji mean?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emogi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'What does the \\xf0\\x9f\\x8d\\x86 \\xf0\\x9f\\x92\\xa6 emoji mean?'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emogi_text.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spellings_Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\acer\\anaconda31\\lib\\site-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\acer\\anaconda31\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\acer\\anaconda31\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\acer\\anaconda31\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\acer\\anaconda31\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\acer\\anaconda31\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\acer\\anaconda31\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Text_with_Errors  \\\n",
      "0  Thiss is a samplee sentence withh speling errr...   \n",
      "1  I havvee somme speleng mistakess in this sente...   \n",
      "\n",
      "                                   Corrected_Text  \n",
      "0  this is a sample sentence with spelling errors  \n",
      "1  I have some spelling mistakes in this sentence  \n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Sample DataFrame with a column containing text with spelling errors\n",
    "data = {'Text_with_Errors': [\"Thiss is a samplee sentence withh speling errrors.\",\n",
    "                              \"I havvee somme speleng mistakess in this sentencc.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Define a function to correct spelling errors in a string\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    for word in text.split():\n",
    "        corrected_text.append(spell.correction(word))\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "# Apply the correct_spelling function to the entire column\n",
    "df['Corrected_Text'] = df['Text_with_Errors'].apply(correct_spelling)\n",
    "\n",
    "# Print the DataFrame with the corrected column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another library for spelling checks -- text blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Text_with_Errors  \\\n",
      "0  Thiss is a samplee sentence withh speling errr...   \n",
      "1  I havvee somme speleng mistakess in this sente...   \n",
      "\n",
      "                                    Corrected_Text  \n",
      "0  Hiss is a sample sentence with spelling errors.  \n",
      "1    I have some spleen mistakes in this sentence.  \n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with a column containing text with spelling errors\n",
    "data = {'Text_with_Errors': [\"Thiss is a samplee sentence withh speling errrors.\",\n",
    "                              \"I havvee somme speleng mistakess in this sentencc.\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to correct spelling errors in a string\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    for word in text.split():\n",
    "        corrected_text.append(str(TextBlob(word).correct()))\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "# Apply the correct_spelling function to the entire column\n",
    "df['Corrected_Text'] = df['Text_with_Errors'].apply(correct_spelling)\n",
    "\n",
    "# Print the DataFrame with the corrected column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords : Stop words are common words that are often filtered out during text preprocessing in natural language processing (NLP) tasks. These words are considered to be non-informative or redundant in the context of analyzing text data because they occur frequently across documents and do not carry significant meaning. Examples of stop words include articles (e.g., \"a\", \"an\", \"the\"), prepositions (e.g., \"in\", \"on\", \"at\"), conjunctions (e.g., \"and\", \"or\", \"but\"), and common verbs (e.g., \"is\", \"are\", \"have\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  This is a sample sentence demonstrating the re...   \n",
      "1  Stop words are common words that are filtered ...   \n",
      "2  Removing stop words helps reduce dimensionalit...   \n",
      "\n",
      "                              Text_without_stopwords  \n",
      "0  sample sentence demonstrating removal stop wor...  \n",
      "1  Stop words common words filtered text preproce...  \n",
      "2  Removing stop words helps reduce dimensionalit...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample DataFrame with a column containing text data\n",
    "data = {'Text': [\"This is a sample sentence demonstrating the removal of stop words.\",\n",
    "                 \"Stop words are common words that are filtered out during text preprocessing.\",\n",
    "                 \"Removing stop words helps reduce dimensionality and improve efficiency in NLP tasks.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Download NLTK stopwords (you need to do this only once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize function with stop word removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenize_and_remove_stopwords = lambda text: ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_words])\n",
    "\n",
    "# Apply tokenize_and_remove_stopwords function to the entire column\n",
    "df['Text_without_stopwords'] = df['Text'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Print the DataFrame with the column containing text without stop words\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emogi --- To remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Text_with_Emojis    Text_without_Emojis\n",
      "0       I love Python! üòçüêç        I love Python! \n",
      "1  Just received a gift üéÅ  Just received a gift \n",
      "2   Feeling happy today üòÄ   Feeling happy today \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample DataFrame with a column containing text data with emojis\n",
    "data = {'Text_with_Emojis': [\"I love Python! üòçüêç\", \"Just received a gift üéÅ\", \"Feeling happy today üòÄ\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to remove emojis from a string\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the remove_emojis function to the entire column\n",
    "df['Text_without_Emojis'] = df['Text_with_Emojis'].apply(remove_emojis)\n",
    "\n",
    "# Print the DataFrame with the new column containing text without emojis\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does the :eggplant: :sweat_droplets: emoji mean? :fire: Fire Emoji\n"
     ]
    }
   ],
   "source": [
    "# Or can keep emoji in terms of text\n",
    "import emoji\n",
    "print(emoji.demojize(\"What does the üçÜ üí¶ emoji mean? üî• Fire Emoji\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation : Tokenization is the process of breaking down text into smaller units, typically words or subwords, which are called tokens. These tokens serve as the basic units of analysis in natural language processing (NLP) tasks. Tokenization is a fundamental step in NLP preprocessing pipelines, as it allows computers to understand and process human language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'does', 'the', 'emoji', 'mean', '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    split=re.split(\"\\W+\",text) \n",
    "    return split\n",
    "tokenize(\"What does the emoji mean?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Use split function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'goint', 'to', 'delhi']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I am goint to delhi\"\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi', ' I am work at an office', ' I am yash']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Sentence Split\n",
    "sent2 = \"I am going to delhi. I am work at an office. I am yash\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems with Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'New', 'Delhi!!!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3 = \"I am going to New Delhi!!!\"\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do you go ? How r u']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do you go ? How r u'\n",
    "sent4.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent = \"I am going to# delhi!\"\n",
    "tokens = re.findall(\"[\\w']+\", sent)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Using ibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi', '!', '!', '!']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I am going to delhi !!!\"\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Mr', '.', 'I', 'am', 'going', 'to', 'do', 'P.Hd', 'in', 'A.I']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Hi Mr. I am going to do P.Hd in A.I\"\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Give', 'me', '100', '$']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Give me 100$\"\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " ',',\n",
       " 'mail',\n",
       " 'me',\n",
       " 'at',\n",
       " 'xyz',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = \"I am here to help, mail me at xyz@gmail.com\"\n",
    "word_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '50km', 'ride', 'costs', 'something', 'around', '$', '50']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3 = \"A 50km ride costs something around $50\"\n",
    "word_tokenize(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Best library name Spacy -- recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  Tokenization is the process of breaking down t...   \n",
      "1  Each unit, typically words or subwords, is cal...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [Tokenization, is, the, process, of, breaking,...  \n",
      "1  [Each, unit, ,, typically, words, or, subwords...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample DataFrame with a column containing text data\n",
    "data = {'Text': [\"Tokenization is the process of breaking down text into smaller units.\",\n",
    "                 \"Each unit, typically words or subwords, is called a token.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function for tokenization\n",
    "def tokenize_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract tokens from the processed document\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization function to the entire column\n",
    "df['Tokens'] = df['Text'].apply(tokenize_text)\n",
    "\n",
    "# Print the DataFrame with the new column containing tokens\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming : Stemming is the process of reducing a word to its base or root form, known as the stem. It involves removing suffixes or prefixes from words to extract the core meaning. The goal of stemming is to normalize words so that variations of the same word are treated as the same word, regardless of their grammatical forms.\n",
    "\n",
    "Inflection : In very simple terms, inflection is when we change a word to show different meanings or uses. For example, when we add \"ed\" to \"walk\" to make \"walked\" to show that it happened in the past. Or when we add \"s\" to \"cat\" to make \"cats\" to show there's more than one. It's like changing the shape or form of a word to fit different situations or roles in a sentence.\n",
    "\n",
    "Used mostly in information retrival\n",
    "\n",
    "It is not 100 % correct so if we want more accuracy we can use lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0                                     running fishes   \n",
      "1                                       cats playing   \n",
      "2  I will start walking a bit fast tomorrow becau...   \n",
      "\n",
      "                                        Stemmed_Text  \n",
      "0                                           run fish  \n",
      "1                                           cat play  \n",
      "2  i will start walk a bit fast tomorrow becaus w...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample DataFrame with a column containing text data\n",
    "data = {'Text': [\"running fishes\", \"cats playing\",\"I will start walking a bit fast tomorrow because walking is good for health.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function for stemming\n",
    "def stem_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Stem each word in the text\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Join the stemmed words into a single string\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text\n",
    "\n",
    "# Apply stemming function to the entire column\n",
    "df['Stemmed_Text'] = df['Text'].apply(stem_text)\n",
    "\n",
    "# Print the DataFrame with the new column containing stemmed text\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lammetiser -- Slower than stemming but more accurate\n",
    "\n",
    " Lemmatisation is the process of reducing words to their base or dictionary form, known as the lemma. It's like finding the root word for variations of the same word.\n",
    "\n",
    "Whether lemmatization or stemming is better depends on the specific task and the requirements of your NLP application. Here's a comparison of both:\n",
    "\n",
    "Lemmatization:\n",
    "\n",
    "Lemmatization generally produces more accurate results compared to stemming because it maps words to their dictionary forms (lemmas), which are valid words.\n",
    "Lemmatization takes into account the context of the word and its part of speech, resulting in more meaningful lemmas.\n",
    "However, lemmatization is computationally more expensive and slower compared to stemming.\n",
    "Stemming:\n",
    "\n",
    "Stemming is faster and computationally less expensive compared to lemmatization because it applies simple rules to chop off prefixes or suffixes.\n",
    "Stemming may produce stems that are not actual words, leading to ambiguity or loss of meaning.\n",
    "Stemming is less accurate compared to lemmatization, especially in languages with complex morphology.\n",
    "\n",
    "In general, if you need higher precision and accuracy in your NLP task, and you can afford the computational cost, lemmatization is a better choice. However, if speed and efficiency are more critical, and you can tolerate some loss of precision, stemming may be sufficient.\n",
    "\n",
    "Ultimately, it's essential to experiment with both techniques and evaluate their performance in the context of your specific NLP task to determine which one works better for your application. Additionally, you may also consider hybrid approaches that combine both stemming and lemmatization to achieve a balance between accuracy and efficiency.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Text Lemmatized_Text\n",
      "0  running fishes    running fish\n",
      "1    cats playing     cat playing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample DataFrame with a column containing text data\n",
    "data = {'Text': [\"running fishes\", \"cats playing\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Lemmatize each word in the text\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the lemmatized words into a single string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply lemmatization function to the entire column\n",
    "df['Lemmatized_Text'] = df['Text'].apply(lemmatize_text)\n",
    "\n",
    "# Print the DataFrame with the new column containing lemmatized text\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
